% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/preprocess_cds.R
\name{preprocess_cds}
\alias{preprocess_cds}
\title{Project a cell_data_set object into a lower dimensional PCA (or ISI) space
after normalize the data}
\usage{
preprocess_cds(cds, method = c("PCA", "tfidf", "none"), num_dim = 50,
  norm_method = c("log", "none"), residual_model_formula_str = NULL,
  pseudo_count = NULL, scaling = TRUE, verbose = FALSE, ...)
}
\arguments{
\item{cds}{the cell_data_set upon which to perform this operation}

\item{method}{the initial dimension method to use, current either PCA or
LSI. For LSI (latent semantic indexing), it converts the (sparse)
expression matrix into tf-idf (term-frequency-inverse document frequency
which increases proportionally to the gene expression value appears in the
cell and is offset by the frequency of the gene in the entire dataset,
which helps to adjust for the fact that some gene appear more frequently
across cell in general.) matrix and then performs SVD to decompose the
gene expression / cells into certain modules / topics. This method can be
used to find associated gene modules and cell clusters at the same time.
It removes noise in the data and thus makes the UMAP result even better.}

\item{num_dim}{the dimensionality of the reduced space. Ignored if
method = "none".}

\item{norm_method}{Determines how to transform expression values prior to
reducing dimensionality}

\item{residual_model_formula_str}{A model formula specifying the effects to
subtract from the data before clustering.}

\item{pseudo_count}{amount to increase expression values before
dimensionality reduction. If NULL (default), pseudo_count of 1 is added
for log normalization and 0 is added for no normalization.}

\item{scaling}{When this argument is set to TRUE (default), it will scale
each gene before running trajectory reconstruction. Relevant for
method = PCA only.}

\item{verbose}{Whether to emit verbose output during dimensionality
reduction}

\item{...}{additional arguments to pass to the dimensionality reduction
function}
}
\value{
an updated cell_data_set object
}
\description{
For most analysis (including trajectory inference, clustering)
in Monocle 3, it requires us to to start from a low dimensional PCA space.
\code{preprocess_cds} will be used to first project a cell_data_set object
into a lower dimensional PCA space before we apply clustering with community
detection algorithm or other non-linear dimension reduction method, for
example UMAP, tSNE, etc.  While tSNE is especially suitable for visualizing
clustering results, comparing to UMAP, the global distance in tSNE space is
not meaningful. UMAP can either be used for visualizing clustering result or
as a general non-linear dimension reduction method. SimplePPT, DDRTree and
L1-graph are two complementary trajectory inference method where the first
one is very great at learning a tree structure but the later is general and
can learn any arbitrary graph structure. Both methods can be applied to the
UMAP space.
}
\details{
Data will still be size_factor_normalized!

In Monocle3, we overhauled the code from Monocle2 so that a standard
Monocle3 workflow works as following:
\enumerate{
\item run \code{preprocess_cds} to project a cell_data_set object into a lower
dimensional PCA space after normalizing the data
\item run \code{reduce_dimension} to further project the PCA space into much
lower dimension space with non-linear dimension reduction techniques,
including tSNE, UMAP.
\item run \code{partition_cells} to partition cells into different graphs based
on a similar approach proposed by Alex Wolf and colleagues. We then
reconstruct the trajectory in each partition with the \code{learn_graph}
function.
\item run \code{learn_graph} to reconstruct developmental trajectory with
reversed graph embedding algorithms. In monocle 3, we enabled the capability
to learn multiple disjointed trajectory with either tree or loop structure,
etc.
}

Prior to reducing the dimensionality of the data, it usually helps to
normalize it so that highly expressed or highly variable genes don't
dominate the computation. \code{reduce_dimension()} automatically transforms
the data in one of several ways depending on the \code{expression_family} of
the cell_data_set object. If the expression_family is \code{"negbinomial"}
or \code{"negbinomial.size"}, the data are variance-stabilized. If the
expression_family is \code{"Tobit"}, the data are adjusted by adding a
pseudocount (of 1 by default) and then log-transformed. If you don't want
any transformation at all, set \code{norm_method} to "none" and
\code{pseudo_count} to 0. This maybe useful for single-cell qPCR data, or
data you've already transformed yourself in some way.
}
