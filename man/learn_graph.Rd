% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learn_graph.R
\name{learn_graph}
\alias{learn_graph}
\title{Learn principal graph from the reduced dimension space using reversed graph
embedding}
\usage{
learn_graph(
  cds,
  use_partition = TRUE,
  close_loop = TRUE,
  learn_graph_control = NULL,
  verbose = FALSE
)
}
\arguments{
\item{cds}{the cell_data_set upon which to perform this operation}

\item{use_partition}{logical parameter that determines whether to use
partitions calculated during \code{cluster_cells} and therefore to learn
disjoint graph in each partition. When \code{use_partition = FALSE}, a
single graph is learned across all partitions. Default is TRUE.}

\item{close_loop}{logical parameter that determines whether or not to
perform an additional run of loop closing after estimating the principal
graphs to identify potential loop structure in the data space. Default is
TRUE.}

\item{learn_graph_control}{NULL or a list of control parameters to be
passed to the reversed graph embedding function. Default is NULL. A list
of potential control parameters is provided in details.}

\item{verbose}{Whether to emit verbose output during graph learning.}
}
\value{
an updated cell_data_set object
}
\description{
Monocle3 aims to learn how cells transition through a
biological program of gene expression changes in an experiment. Each cell
can be viewed as a point in a high-dimensional space, where each dimension
describes the expression of a different gene. Identifying the program of
gene expression changes is equivalent to learning a \emph{trajectory} that
the cells follow through this space. However, the more dimensions there are
in the analysis, the harder the trajectory is to learn. Fortunately, many
genes typically co-vary with one another, and so the dimensionality of the
data can be reduced with a wide variety of different algorithms. Monocle3
provides two different algorithms for dimensionality reduction via
\code{reduce_dimension} (UMAP and tSNE). Both take a cell_data_set object
and a number of dimensions allowed for the reduced space. You can also
provide a model formula indicating some variables (e.g. batch ID or other
technical factors) to "subtract" from the data so it doesn't contribute to
the trajectory. The function \code{learn_graph} is the fourth step in the
trajectory building process after \code{preprocess_cds},
\code{reduce_dimension}, and \code{cluster_cells}. After
\code{learn_graph}, \code{order_cells} is typically called.
}
\section{Optional \code{learn_graph_control} parameters}{

\describe{
\item{euclidean_distance_ratio:}{The maximal ratio between the euclidean
distance of two tip nodes in the spanning tree and the maximum distance
between any connecting points on the spanning tree allowed to be connected
during the loop closure procedure. Default is 1.}
\item{geodesic_distance_ratio:}{The minimal ratio between the geodesic
distance of two tip nodes in the spanning tree and the length of the
diameter path on the spanning tree allowed to be connected during the loop
closure procedure. (Both euclidean_distance_ratio and
geodesic_distance_ratio need to be satisfied to introduce the edge for
loop closure). Default is 1/3.}
\item{minimal_branch_len:}{The minimal length of the diameter path for a
branch to be preserved during graph pruning procedure. Default is 10.}
\item{orthogonal_proj_tip:}{ Whether to perform orthogonal projection for
cells corresponding to the tip principal points. Default is FALSE.}
\item{prune_graph:}{Whether or not to perform an additional round of graph
pruning to remove small insignificant branches. Default is TRUE.}
\item{scale:}{}
\item{ncenter:}{}
\item{nn.k:}{Maximum number of nearest neighbors to compute in the
reversed graph embedding. Set k=NULL
to let learn_graph estimate k. Default is 25.}
\item{rann.k:}{nn.k replaces rann.k but rann.k is available for
compatibility with existing code.}
reversed graph embedding. Set rann.k=NULL to let learn_graph estimate
rann.k. Default is 25.
\item{maxiter:}{}
\item{eps:}{}
\item{L1.gamma:}{}
\item{L1.sigma:}{}
\item{nn.method:}{The package to use for finding nearest neighbors in
the reverse graph embedding. nn.method can be one of 'nn2', 'annoy', or
'hnsw'. Default is 'nn2'.}
\item{nn.metric:}{The distance metric for the annoy or hnsw nearest
neighbor index build in the reverse graph embedding. See help(set_nn_control)
for more information. Default is 'euclidean'.}
\item{nn.n_trees:}{The number of trees used to build the annoy nearest
neighbor index in the reverse graph embedding. See help(set_nn_control)
for more information. Default is 50.}
\item{nn.search_k:}{The number of nodes to search in the annoy index
search in the reverse graph embedding. See help(set_nn_control) for
more information. Default is 100 * nn.k.}
\item{nn.M:}{Related to internal dimensionality of HNSW index in the
reverse graph embedding. See help(set_nn_control) for more information.
Default is 48.}
\item{nn.ef_construction:}{Controls the HNSW index build speed/accuracy
tradeoff in the reverse graph embedding. Default is 200.}
\item{nn.ef:}{Controls the HNSW index search speed/accuracy tradeoff in
the reverse graph embedding. See help(set_nn_control) for more
information. Default is 10.}
\item{nn.grain_size:}{Used by annoy and HNSW to set the minimum amount
of work to do per thread in the reverse graph embedding. See
help(set_nn_control) for more information. Default is 1.}
\item{nn.cores:}{Used by annoy and HNSW to control the number of
threads used in the reverse graph embedding. See help(set_nn_control)
for more information. Default is 1.}
}
}

